{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-36-63e42d658f6b>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-63e42d658f6b>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    class data_gatherer():\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "# 한 파일을 parse하여 title, data배열을 얻어냄.\n",
    "class poem_parser():\n",
    "    def parse(text):\n",
    "        title = list()\n",
    "        data = list()\n",
    "        start = text.find('POEM_TITLE\\n')\n",
    "        while(start != -1):\n",
    "            text = text[start+len('POEM_TITLE\\n'):]\n",
    "            middle = text.find('POEM_DATA\\n')\n",
    "            start = text.find('POEM_TITLE\\n')\n",
    "            \n",
    "            title.append(text[:middle-1])\n",
    "            if(start == -1):\n",
    "                data.append(text[middle+len('POEM_DATA\\n'):-1])\n",
    "            else:\n",
    "                data.append(text[middle+len('POEM_DATA\\n'):start-1])\n",
    "        return title, data\n",
    "    def parse_seq(text, seq_len):\n",
    "        title = list()\n",
    "        data = list()\n",
    "        start = text.find('POEM_TITLE\\n')\n",
    "        while(start != -1):\n",
    "            text = text[start+len('POEM_TITLE\\n'):]\n",
    "            middle = text.find('POEM_DATA\\n')\n",
    "            start = text.find('POEM_TITLE\\n')\n",
    "            \n",
    "            title.append(text[:middle-1])\n",
    "            if(start == -1):\n",
    "                data.append(text[middle+len('POEM_DATA\\n'):-1])\n",
    "            else:\n",
    "                data.append(text[middle+len('POEM_DATA\\n'):start-1])\n",
    "            # data cutting by sequence\n",
    "            # TODO\n",
    "            \n",
    "            \n",
    "        return title, data\n",
    "\n",
    "# 여러 파일에 나뉘어져있는 데이터를 한 파일로 합치기 위한 모듈\n",
    "# clean : 저장 할 파일을 초기화\n",
    "# gather('a.txt', 'b.txt', ...) : args로 주어진 파일들을 읽어 대상파일로 저장\n",
    "class data_gatherer():\n",
    "    def __init__(self, encoding='utf-8'):\n",
    "        self.poem_set_connect = 'poem_set_connect.txt'\n",
    "        self.poem_set = 'poem_set.txt'\n",
    "        self.title_set_connect = 'title_set_connect.txt'\n",
    "        self.title_set = 'title_set.txt'\n",
    "        self.encoding = encoding\n",
    "    def clean(self):\n",
    "        f = codecs.open(self.poem_set, \"w\", self.encoding)\n",
    "        f.close()\n",
    "        f = codecs.open(self.title_set, \"w\", self.encoding)\n",
    "        f.close()\n",
    "    def clean_connect(self):\n",
    "        f = codecs.open(self.poem_set_connect, \"w\", self.encoding)\n",
    "        f.close()\n",
    "        f = codecs.open(self.title_set_connect, \"w\", self.encoding)\n",
    "        f.close()\n",
    "    def gather(self, path_list):\n",
    "        f1 = codecs.open(self.poem_set, \"a\", self.encoding)\n",
    "        for path in path_list:\n",
    "            with codecs.open(path, \"r\", encoding=self.encoding) as f2:\n",
    "                text = f2.read()\n",
    "                # do text processing\n",
    "                data = text\n",
    "                f1.write(data)\n",
    "            f2.close()\n",
    "        f1.close()\n",
    "    def gather_connect(self, path_list):\n",
    "        f1 = codecs.open(self.poem_set_connect, \"a\", self.encoding)\n",
    "        f3 = codecs.open(self.title_set_connect, \"a\", self.encoding)\n",
    "        for path in path_list:\n",
    "            with codecs.open(path, \"r\", encoding=self.encoding) as f2:\n",
    "                text = f2.read()\n",
    "                # do text processing\n",
    "                titles, data = poem_parser.parse(text)\n",
    "                for i in range(len(data)):\n",
    "                    datum = data[i]\n",
    "                    title = titles[i]\n",
    "                    title_pos = len(datum)\n",
    "                    if(datum[-1]!='\\n'):\n",
    "                        datum += '\\n'\n",
    "                        title_pos += 1\n",
    "                    f1.write(datum)\n",
    "                    f3.write(title+'\\n'+str(title_pos)+'\\n')\n",
    "            f2.close()\n",
    "        f1.close()\n",
    "        f3.close()\n",
    "\n",
    "# 데이터를 로드, 전처리, 배치 생성을 위함\n",
    "# https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/utils.py\n",
    "# https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "# 참조\n",
    "class data_loader():\n",
    "    def __init__(self, seq_len, batch, poem_collection=\"poem_collection.txt\", encoding='utf-8'):\n",
    "        self.poem_collection = poem_collection\n",
    "        self.seq_len = seq_len\n",
    "        self.batch = batch\n",
    "        self.encoding = encoding\n",
    "    def data_preprocess():\n",
    "        return\n",
    "    def data_load():\n",
    "        return\n",
    "    def make_batch():\n",
    "        return\n",
    "    \n",
    "x = np.arange(100)\n",
    "batch_size = 10\n",
    "seq_length = 3\n",
    "num_batch = int(len(x) / (batch_size * seq_length))\n",
    "\n",
    "# 딱 나누어 떨어지지 않는 경우, 남겨지는 부분을 버림.\n",
    "# https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/utils.py 의 이해를 위하여 작성해본 코드.\n",
    "x = x[:num_batch * batch_size * seq_length]\n",
    "x_batches = np.split(x.reshape(batch_size, -1), num_batch, 1)\n",
    "print(x_batches[0])\n",
    "print(x_batches[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계획\n",
    "# https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/ 를 참고하여\n",
    "# input의 각 글자는 알파벳 총 개수 + 1만큼의 길이를 가진 vector로 표현한다.\n",
    "# 실험적 : vector 마지막 값은 시작으로부터의 길이값을 나타낸다. 길이 정보를 넣어주었을때 어느정도 선에서\n",
    "#          시를 끊을 지 학습 할 수 있을 것이라고 생각했기 때문이다.\n",
    "# 각 vector 원소는 각 알파벳을 나타낸다.\n",
    "# [0, 0, 0, 0, ... 0] 은 아무것도 아닌 글자를 나타낸다. 즉 아무것도 없는 상태의 시작, 또는 종료, 또는 종료 이후의 글자이다.\n",
    "gatherer = data_gatherer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatherer.clean()\n",
    "gatherer.clean_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatherer.gather_connect(['text1.txt', 'text2.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
